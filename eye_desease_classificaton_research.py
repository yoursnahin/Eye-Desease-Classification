# -*- coding: utf-8 -*-
"""Eye Desease Classificaton Research (Thesis).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HBivvEsbvgSM-D6VUMNlRozgwPjnUOL2
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras import datasets,layers,models
import matplotlib.pyplot as plt
import numpy as np

import os

data_dir = "/content/drive/MyDrive/Eye disease image dataset/Original Dataset"

for class_name in os.listdir(data_dir):
    class_path = os.path.join(data_dir, class_name)
    if os.path.isdir(class_path):
        count = len([f for f in os.listdir(class_path) if f.lower().endswith(('jpg', 'jpeg', 'png'))])
        print(f"{class_name}: {count} images")

dataset = tf.keras.utils.image_dataset_from_directory(
    "/content/drive/MyDrive/Eye disease image dataset/Original Dataset",
    labels="inferred",
    label_mode= "categorical",
    class_names=None,
    color_mode="rgb",
    batch_size=32,
    image_size=(224, 224),
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation="bilinear",
    follow_links=False,
    crop_to_aspect_ratio=False,
    pad_to_aspect_ratio=False,
    data_format=None,
    verbose=True,

)

for images, labels in dataset.take(1):
    print("Image batch shape:", images.shape)

"""Splitting into train dataset and validation dataset"""

image_dir = '/content/drive/MyDrive/Eye disease image dataset/Original Dataset'

# Training dataset (80% of data)
train_ds = tf.keras.utils.image_dataset_from_directory(
    image_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(224, 224),
    batch_size=32,
    label_mode='categorical'  # for class_weight later
)

# Validation dataset (20% of data)
val_ds = tf.keras.utils.image_dataset_from_directory(
    image_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(224, 224),
    batch_size=32,
    label_mode='categorical'
)

print(train_ds.class_names)

"""**Applying model on raw data**
1. Apply VGG16 model

"""

import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras import layers, models

# 1ï¸âƒ£ Load the pretrained VGG16 base model
base_model = VGG16(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
)

# Freeze the convolutional base
base_model.trainable = False

# 2ï¸âƒ£ Add your custom classification head
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.3),
    layers.Dense(256, activation='relu'),
    layers.Dense(10, activation='softmax')  # 10 disease classes
])

# 3ï¸âƒ£ Compile the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',  # because label_mode='categorical'
    metrics=['accuracy']
)

# 4ï¸âƒ£ Train (fit) the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,    # start small; increase after confirming stability
    verbose=1
)

# 5ï¸âƒ£ Evaluate the model
val_loss, val_acc = model.evaluate(val_ds)
print(f"âœ… Validation Accuracy: {val_acc:.4f}")
print(f"ðŸ“‰ Validation Loss: {val_loss:.4f}")

"""1.1 Evaluation"""

# (Precision, Recall, F1
from sklearn.metrics import classification_report, confusion_matrix

# Get class names from dataset
class_names = list(train_ds.class_names)

# Collect predictions and true labels
y_true = []
y_pred = []

for images, labels in val_ds:
    preds = model.predict(images)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

# Generate detailed report
print(classification_report(y_true, y_pred, target_names=class_names))

#Confusion Matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names,
            yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix for VGG16')
plt.show()

# Training Performance Curves (Accuracy & Loss over Epochs)
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title('VGG16 Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('VGG16 Loss')
plt.show()

"""2. GoogleNet"""

import tensorflow as tf
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# âœ… Load pretrained InceptionV3 (modern GoogLeNet)
base_model = InceptionV3(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
)

# Freeze base model to keep pretrained features
base_model.trainable = False

# âœ… Add custom classifier
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.3),
    layers.Dense(256, activation='relu'),
    layers.Dense(10, activation='softmax')  # 10 classes in your dataset
])

# âœ… Compile model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',  # because label_mode='categorical'
    metrics=['accuracy']
)

# âœ… Train model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,     # increase later for fine-tuning
    verbose=1
)
val_loss, val_acc = model.evaluate(val_ds)
print(f"âœ… Validation Accuracy: {val_acc:.4f}")
print(f"ðŸ“‰ Validation Loss: {val_loss:.4f}")

"""2.1 Evaluation"""

# Classification Report
class_names = list(train_ds.class_names)
y_true, y_pred = [], []

for images, labels in val_ds:
    preds = model.predict(images)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

print(classification_report(y_true, y_pred, target_names=class_names))

# Confusino Matrix
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names,
            yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - GoogLeNet (InceptionV3)')
plt.show()

# Training Curve
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title('GoogLeNet (InceptionV3) Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('GoogLeNet (InceptionV3) Loss')
plt.show()

"""3. ResNet-50

"""

import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# âœ… Load pretrained ResNet-50 (ImageNet weights)
base_model = ResNet50(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
)

# Freeze convolutional base (transfer learning)
base_model.trainable = False

# âœ… Add your custom classification head
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.3),
    layers.Dense(256, activation='relu'),
    layers.Dense(10, activation='softmax')   # 10 classes in your dataset
])

# âœ… Compile the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',   # because label_mode='categorical'
    metrics=['accuracy']
)

# âœ… Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,      # adjust later if needed
    verbose=1
)

"""3.1 Evaluation"""

val_loss, val_acc = model.evaluate(val_ds)
print(f"âœ… Validation Accuracy: {val_acc:.4f}")
print(f"ðŸ“‰ Validation Loss: {val_loss:.4f}")

# Recall, F1, Precision
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

class_names = list(train_ds.class_names)
y_true, y_pred = [], []

for images, labels in val_ds:
    preds = model.predict(images)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

print(classification_report(y_true, y_pred, target_names=class_names))

# Confusion Matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names,
            yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - ResNet50')
plt.show()

# Training Curve
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title('ResNet50 Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('ResNet50 Loss')
plt.show()

"""4. XceptionNet"""

import tensorflow as tf
from tensorflow.keras.applications import Xception
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# âœ… Load pretrained Xception model
base_model = Xception(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
)

# Freeze convolutional base for transfer learning
base_model.trainable = False

# âœ… Build the model
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.3),
    layers.Dense(256, activation='relu'),
    layers.Dense(10, activation='softmax')   # your 10 classes
])

# âœ… Compile the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# âœ… Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,   # tune later if needed
    verbose=1
)
val_loss, val_acc = model.evaluate(val_ds)
print(f"âœ… Validation Accuracy: {val_acc:.4f}")
print(f"ðŸ“‰ Validation Loss: {val_loss:.4f}")

#F1 Precision Call
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

class_names = list(train_ds.class_names)
y_true, y_pred = [], []

for images, labels in val_ds:
    preds = model.predict(images)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

print(classification_report(y_true, y_pred, target_names=class_names))

# Confusion Matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names,
            yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - XceptionNet')
plt.show()

# Training Curve
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title('XceptionNet Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('XceptionNet Loss')
plt.show()

"""#EfficientNetB3"""

import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB3
from tensorflow.keras import layers, models
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Base model (transfer learning)
base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze convolutional base

# Build model
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.3),
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(len(train_ds.class_names), activation='softmax')
])

# Compile
model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# --- HANDLE IMBALANCE ---
class_names = train_ds.class_names
class_counts = [139, 125, 444, 500, 1024, 1349, 17, 127, 101, 1509]

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.arange(len(class_names)),
    y=np.concatenate([np.full(c, i) for i, c in enumerate(class_counts)])
)
class_weights = dict(enumerate(class_weights))
print("Class Weights:", class_weights)

# --- TRAIN MODEL ---
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    class_weight=class_weights
)

from sklearn.metrics import classification_report
import numpy as np

# Get class names
class_names = val_ds.class_names

# Get true labels and predicted labels
y_true = np.concatenate([np.argmax(y, axis=1) for x, y in val_ds])
y_pred = np.argmax(model.predict(val_ds), axis=1)
# Generate Precision, Recall, F1-score, and Support
report = classification_report(y_true, y_pred, target_names=class_names)
print(report)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Normalize the matrix (optional, to see percentage instead of counts)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

# Plot
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues',
xticklabels=class_names, yticklabels=class_names)

plt.xlabel("Predicted Labels", fontsize=12)
plt.ylabel("True Labels", fontsize=12)
plt.title("Normalized Confusion Matrix", fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# Training Curve for EfficientNetB3
plt.figure(figsize=(12, 4))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title('EfficientNetB3 Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('EfficientNetB3 Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.show()

# Model performance metrics (replace with your results)
import pandas as pd

data = {
    'Model': ['VGG16', 'GoogLeNet', 'ResNet50', 'EfficientNetB3'],
    'Accuracy': [0.91, 0.90, 0.94, 0.96],
    'Precision': [0.89, 0.88, 0.93, 0.95],
    'Recall': [0.88, 0.87, 0.92, 0.94],
    'F1-Score': [0.88, 0.87, 0.92, 0.95],
    'AUC': [0.92, 0.91, 0.95, 0.97]
}

# Create DataFrame
df = pd.DataFrame(data)
df = df.set_index('Model')

# Display DataFrame
print(df)

"""#Augmentation"""

import os
import cv2
import random
import albumentations as A
from tqdm import tqdm

# ==============================
# PATH CONFIGURATION
# ==============================
INPUT_DIR = "/content/drive/MyDrive/Eye disease image dataset/Original Dataset"
OUTPUT_DIR = "/content/drive/MyDrive/Eye disease image dataset/Augmented Dataset"
TARGET_COUNT = 1000   # target number of images per class
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ==============================
# DEFINE RETINA-SAFE AUGMENTATION PIPELINE
# ==============================
transform = A.Compose([
    #  Safe for retinal fundus images
    A.HorizontalFlip(p=0.5),  # simulate left/right eye
    A.ShiftScaleRotate(
        shift_limit=0.05,      # small shifts (â‰¤5%)
        scale_limit=0.1,       # slight zoom-in/out (Â±10%)
        rotate_limit=15,       # clinically safe rotation (Â±15Â°)
        border_mode=cv2.BORDER_REFLECT_101,
        p=0.8
    ),
    A.RandomBrightnessContrast(
        brightness_limit=0.2,  # simulate camera light variations
        contrast_limit=0.2,
        p=0.5
    ),
    A.GaussianBlur(blur_limit=3, p=0.3),          # realistic blur
    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),  # simulate sensor noise
])

# ==============================
# MAIN LOOP â€” AUGMENT SMALL CLASSES
# ==============================
for class_name in os.listdir(INPUT_DIR):
    class_path = os.path.join(INPUT_DIR, class_name)
    if not os.path.isdir(class_path):
        continue

    # List valid image files
    images = [f for f in os.listdir(class_path)
              if f.lower().endswith(('jpg', 'jpeg', 'png'))]
    count = len(images)

    print(f"\n {class_name}: {count} images")

    # Skip already large classes
    if count >= TARGET_COUNT:
        print(f" Skipping (already has {count} â‰¥ {TARGET_COUNT})")
        continue

    # Create output folder for augmented images
    save_dir = os.path.join(OUTPUT_DIR, class_name)
    os.makedirs(save_dir, exist_ok=True)

    # Copy original images first
    for img_name in images:
        src = os.path.join(class_path, img_name)
        dst = os.path.join(save_dir, img_name)
        img = cv2.imread(src)
        if img is not None:
            cv2.imwrite(dst, img)

    # Augment to reach target count
    num_to_generate = TARGET_COUNT - count
    print(f" Augmenting {num_to_generate} new images for {class_name}")
    i = 0
    pbar = tqdm(total=num_to_generate, desc=f"Augmenting {class_name}")

    while i < num_to_generate:
        img_name = random.choice(images)
        img_path = os.path.join(class_path, img_name)
        image = cv2.imread(img_path)
        if image is None:
            continue

        # Apply safe retinal transformations
        augmented = transform(image=image)['image']
        aug_name = f"aug_{i}.jpg"
        cv2.imwrite(os.path.join(save_dir, aug_name), augmented)
        i += 1
        pbar.update(1)

    pbar.close()
    print(f" Done: {class_name} now has {TARGET_COUNT} images")

print("\n Augmentation complete! All small classes are now balanced safely for retinal analysis.")

import os

aug_dir = "/content/drive/MyDrive/Eye disease image dataset/Augmented Dataset"

for class_name in os.listdir(aug_dir):
    class_path = os.path.join(aug_dir, class_name)
    if os.path.isdir(class_path):
        count = len([f for f in os.listdir(class_path) if f.lower().endswith(('jpg', 'jpeg', 'png'))])
        print(f"{class_name}: {count} images")